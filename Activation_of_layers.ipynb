{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation_of_layers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjNk4KK_euHy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "import torch.nn.init as init\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6OqISrse6_J",
        "outputId": "3ad29ab5-2304-4fc5-adec-6ee90b302a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "yORjMCC3higy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/drive/My Drive/BIA_MnistTrained/sample_images.pth', 'r') as f:\n",
        "sample_images = torch.load('/drive/My Drive/BIA_MnistTrained/sample_images.pth')\n",
        "sample_images = sample_images.unsqueeze(dim=1).type(torch.double)\n",
        "print(sample_images.shape, sample_images.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrPQCqCJhu0U",
        "outputId": "5bac1d66-2dd2-43cb-f302-862ae453c3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 28, 28]) torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # print(input)\n",
        "        # print(output)\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "7rT5BJIHvdvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "for name, module in net.named_children():\n",
        "    print(name, module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyzKcW1fyJMi",
        "outputId": "35e4e509-3dae-4679-fca3-a9e26f7a777c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1 Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "conv2 Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "conv2_drop Dropout2d(p=0.5, inplace=False)\n",
            "fc1 Linear(in_features=320, out_features=50, bias=True)\n",
            "fc2 Linear(in_features=50, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_num = 20\n",
        "for i in range(19, model_num):\n",
        "    at_dir = '/drive/My Drive/BIA_MnistTrained/'\n",
        "    path = os.path.join(at_dir, 'model_{0}.pth').format(i)\n",
        "    model_path = os.path.join(at_dir, 'models_layers/model_{0}').format(i)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path)\n",
        "    weights = torch.load(path)\n",
        "    net = Net()\n",
        "    net.load_state_dict(weights)\n",
        "    image_num = sample_images.shape[0]\n",
        "    activation = {}\n",
        "    layers = {}\n",
        "    for name, module in net.named_children():\n",
        "        layers[name] = torch.tensor([], dtype=float)\n",
        "    with torch.no_grad():\n",
        "        for i in range(image_num):\n",
        "            x = torch.tensor(sample_images[i, :, :, :].clone(), dtype=torch.float)\n",
        "            for name, module in net.named_children():\n",
        "                module.register_forward_hook(get_activation(name))\n",
        "            net_out = net(x)\n",
        "            for name, module in net.named_children():\n",
        "                if layers[name].nelement() == 0:\n",
        "                    layers[name] = torch.cat([layers[name], activation[name]], dim=0)\n",
        "                elif layers[name].shape != activation[name].shape:\n",
        "                    temp = activation[name].unsqueeze(0)\n",
        "                    layers[name] = torch.cat([layers[name], temp], dim=0)\n",
        "                else:\n",
        "                    layers[name] = torch.stack([layers[name], activation[name]], dim=0)\n",
        "    for name, module in net.named_children():\n",
        "        print(os.path.join(model_path, name+'.pth'))\n",
        "        torch.save(layers[name], os.path.join(model_path, name+'.pth'))\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeAPzOAXfW33",
        "outputId": "fa902f38-d0c5-4df5-e3b5-956cd9f49940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/drive/My Drive/BIA_MnistTrained/models_layers/model_19/conv1.pth\n",
            "/drive/My Drive/BIA_MnistTrained/models_layers/model_19/conv2.pth\n",
            "/drive/My Drive/BIA_MnistTrained/models_layers/model_19/conv2_drop.pth\n",
            "/drive/My Drive/BIA_MnistTrained/models_layers/model_19/fc1.pth\n",
            "/drive/My Drive/BIA_MnistTrained/models_layers/model_19/fc2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xncE7zjF9o0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}